---
title: Let's Talk About the Weather - Analyzing Berlinâ€™s Changing Climate.
author: Gershom Tripp
date: '2018-09-16'
slug: let-s-talk-about-the-weather-analyzing-berlin-s-changing-climate
categories:
  - R
  - tidyverse
tags:
  - climate
  - Berlin
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```





### Introduction

What follows is an [EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis) inspired by conversations I've had about Berlin's changing weather, something I've personally witnessed in the 11+ years that I've lived here. Part of my motivation also stems from a running argument obout the direction of that change: My wife thinks June is a sorry excuse for a summer month, and that it used to be nicer, and I think she's imagining things. In the end we might both have to modify our positions a bit. ðŸ˜˜ðŸ˜˜ðŸ˜˜

Beyond mere curosity and [Besserwisserei](https://www.dict.cc/deutsch-englisch/Besserwisser.html), I think it's important to explore how global warming and climate change manifest themselves at the local level. Climate change is happening [now](https://climate.nasa.gov/effects/), and there's some evidence that it's accelerating even [faster than predicted](https://www.theguardian.com/world/2018/aug/21/arctics-strongest-sea-ice-breaks-up-for-first-time-on-record). 

For the record, I'm neither a climatoligist nor a meteorologist, and nothing about my analysis is going to be cutting edge or original. I'll also avoid making any inferences based on the data. That having been said, given the quality of the data I should still be able draw some conclusions.

The next couple of sections cover how I wrangle the data into a more manageable form. If you're not interested in that kind of thing, [skip to the analysis](#analysis).

### Massaging the data

First off, the libraries:

```{r load libs, warning=FALSE, error=FALSE, message=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(stats)
library(lubridate)
library(ggplot2)
```

The data is a subset of the [Deutscher Wetterdienst](https://www.dwd.de/DE/leistungen/klimadatendeutschland/klarchivtagmonat.html)'s
extensive repository of weather data. It's possible to get hourly measurements, going back decades, but I decided daily averages were enough for this analysis. Downloading the data programmatically from the DWD archive interface would have been horribly complex, so I did it the old-fashioned way: I downloaded the data as four ZIPs (from four Berlin weather stations), extracted the TXT/CSV files, renamed them to something more descriptive, and wrote some code to remove some excess white space that was interfering with the data import.

*Edit*: While putting the final touches on this post I discovered that the DWD offers FTP access to their databases via the [Climate Data Center](ftp://ftp-cdc.dwd.de/pub/CDC/).

The data files are semicolon-delimited. To keep things simple I'll import everything as character strings. Numeric and date objects are converted later, as needed. The value `-999` is imported as `NA`.

```{r remove whitespace, message=FALSE}
# get the names of  the data files, import them as text, remove white space and
# overwrite originals with de-whitespaced files
data_path <- file.path("datasets", "berlin_climate_data")
file_names <- dir(data_path)
file_names <- file_names[grep(pattern = "data", file_names)]

for (file_name in file_names){
    file_path <- file.path(data_path, file_name)
    file <- read_file(file_path)
    file <- gsub(" ", "", file)
    write_file(file, file_path)
}

# wrapper function to load data with defaults
read_data <- function(path){
    read_delim(file = path,
               delim = ";",
               col_types = paste0(rep("c", 19), collapse = ""),
               na = "-999"
               )
}

# read in the data
dahlem_fu_data <- read_data(file.path(data_path, "Dahlem-(FU)-data.txt"))
dahlem_lfag_data <- read_data(file.path(data_path, "Dahlem-(LFAG)-data.txt"))
tegel_data <- read_data(file.path(data_path, "Tegel-data.txt"))
tempelhof_data <- read_data(file.path(data_path, "Tempelhof-data.txt"))
```


It makes sense to take a look at the included parameter files to get an idea what kind of data I'm working with. These are in German (umlauts!), so I'll use `guess_encoding()` from the `readr` library to figure out which ISO I need.

```{r guess file encoding}
guess_encoding("datasets/berlin_climate_data/Dahlem-(FU)-params.txt") %>% 
    knitr::kable()
```

ISO-8859-1 it is. I don't care too much about the details of each station, so I'll just import one file, `Tempelhof-params.txt`, to gain some insight into the parameters, i.e. abbreviations, units used, etc. 

```{r load and show params, warning=FALSE}
# load parameter file
data_path <- file.path("datasets", "berlin_climate_data", "Tempelhof-params.txt")
tempelhof_params <- read_delim(file = data_path,
                               delim = ";",
                               col_types = paste0(rep("c", 13), collapse = ""),
                               locale = locale(encoding = "ISO-8859-1")
                               )

# filter and show parameters
tempelhof_params %>%
    select(2:7) %>% 
    filter(!duplicated(Parameter) & !is.na(Parameter)) %>% 
    knitr::kable()
```









### Tidying the data

There are a couple more changes to be made before I can start visualizing. First, [tidyverse](https://www.tidyverse.org/) magic works most potently on *long* data, but the DW uses *wide* format ([see the difference here](https://en.wikipedia.org/wiki/Wide_and_narrow_data)). Second, the variable abbreviations are hard to understand (FM, RSK, etc.), so I should replace them with something else. Third and last, I now have four datasets, one for each Berlin weather station, but one would be preferable.

I'll change the variable names using the following map.

```{r variable names map}
var_names <- c("Date" = "MESS_DATUM",
               "Avg. Wind Speed (m/sec)" = "FM",
               "Max Wind Speed (m/sec)" = "FX",
               "Avg. Cloud Cover (8th)" = "NM",
               "Avg. Air Pressure (hPa)" = "PM",
               "Precipitation (mm)" = "RSK",
               "Sun Shine Duration (h)" = "SDK",
               "Snow (cm)" = "SHK_TAG",
               "Avg. Temp. (Â°C)" = "TMK",
               "Min. Temp. (Â°C)" = "TNK",
               "Max Temp. (Â°C)" = "TXK",
               "Avg. Humidity (%)" = "UPM"
               )
```

The following `tidy_data` function reduces each data frame to the set of variables I'm interested in, converts the character strings to numbers and dates, and `gathers` the variables into long format.

```{r tidy_data function}
tidy_data <- function(df, vars, station_name){
    df <- select(df, vars)
    df <- as_tibble(sapply(df, as.double))
    df$Date <- ymd(df$Date)
    df <- gather(df, key = "Variable", value = "Value", 2:12)
    df$Station <- station_name
    df
}
```

I can now call `tidy_data` for each station, and then combine the station data into one data frame `berlin_data`.

```{r tidy stations}
# call tidy_data for each station dataset
dahlem_lfag_data <- tidy_data(dahlem_lfag_data, var_names, "Dahlem (LFAG)")
dahlem_fu_data <- tidy_data(dahlem_fu_data, var_names, "Dahlem (FU)")
tegel_data <- tidy_data(tegel_data, var_names, "Tegel")
tempelhof_data <- tidy_data(tempelhof_data, var_names, "Tempelhof")

# combine station data into one data frame
berlin_data <- rbind(dahlem_lfag_data, dahlem_fu_data, tegel_data, tempelhof_data)
```

There's just one final change I'd like to make: The values reflect daily averages, and measurement began in the 19th century for some stations, so I'm faced with hundreds of thousands of observations! To make things more manageable I'll include different time scales, e.g. years, months, weeks, days, and [meteorological seasons](https://en.wikipedia.org/wiki/Season#Meteorological). While I'm at it I'll also convert months and seasons to factors.

```{r add year month week season}
# add years, months, weeks, days, and seasons
berlin_data <- berlin_data %>% 
    mutate(Year = year(Date),
           Month = month(Date),
           Week = week(Date),
           Day = day(Date),
           Season = case_when(Month %in% c(12, 1, 2) ~ "Winter",
                              Month %in% 3:5 ~ "Spring",
                              Month %in% 6:8 ~ "Summer",
                              Month %in% 9:11 ~ "Fall",
                              )
           )

# turn seasons into factors
berlin_data$Season <- factor(berlin_data$Season,
                             ordered = T,
                             levels = c("Spring", "Summer", "Winter", "Fall")
                             )

# turn months into factors
berlin_data$Month <- factor(berlin_data$Month,
                            ordered = T,
                            levels = 1:12,
                            labels = c("January", "February", "March", "April",
                                       "May", "June", "July", "August", 
                                       "September", "October", "November",
                                       "December"
                                       )
                            )
```





***



### <a name="analysis"></a>Visualizing Berlin's changing climate

I now have a few hundred thousand observations for each of four Berlin weather stations, which I can freely group based on my analytical needs. The following table shows the total observations (daily averages) for each of Berlin's four weather stations.

```{r show number of observations}
berlin_data %>% 
    group_by(Station) %>% 
    summarise("number of observations" = n()) %>% 
    knitr::kable()
```

Let's first take a look at how average yearly temperatures have changed since 1876. The following time series shows that average yearly temperatures in Berlin have increased by about 2Â°C since the middle of the 20th century, although there is a fair amount of yearly variation.
 
```{r viz mean yearly temps, message=FALSE}
# create variable to hold theme (reusable)
thm <- theme_minimal() + 
    theme(panel.grid.minor = element_blank(),
          panel.grid.major.x = element_blank()
          )

berlin_data %>% 
    filter(Variable == "Avg. Temp. (Â°C)") %>%
    group_by(Station, Year) %>% 
    summarise(Value = mean(Value, na.rm = T)) %>%
    ggplot(aes(x = Year, y = Value)) +  
    geom_line(aes(color = Station)) +
    geom_smooth(se = F, method = "loess", span = .2) +
    labs(title = "Avg. Yearly Temperature (Â°C), 1876-2017",
         y = "Temp."
         ) +
    thm +
    scale_x_continuous(limits = c(1876, 2017),
                       breaks = seq(1877, 2017, by = 10)
                       ) +
    scale_y_continuous(limits = c(3, 12),
                       breaks = seq(3, 12, by = 1)
                       )
```

Also conspicuous is the precipitous drop in 1945. Histograms of each year from 1941-1949 reveal bimodal distributions of daily temperatures for most years. One mode is missing from 1945, however - that is to say, some data is missing - and the data that is available clusters around the low end.

```{r viz distribution of temps 1941-1949}
berlin_data %>% 
    filter(Station == "Dahlem (LFAG)",
           Variable == "Avg. Temp. (Â°C)",
           Year %in% 1941:1949,
           !is.na(Value)
           ) %>% 
    ggplot(aes(x = Value)) + 
    geom_histogram(bins = 30) +
    facet_wrap(~Year) + 
    theme_minimal() + 
    theme(panel.grid.minor = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.spacing = unit(2, "lines")
          ) +
    labs(title = "Distribution of Annual Temperatures (Â°C), 1941-1949",
         x = "Temperature"
         )
```

A table observation counts (i.e. number of daily averages) confirms this.

```{r}
berlin_data %>% 
    filter(Station == "Dahlem (LFAG)",
           Variable == "Avg. Temp. (Â°C)",
           Year %in% 1941:1949,
           !is.na(Value)
           ) %>% 
    group_by(Year) %>% 
    summarize("number of obs." = n(),
              "annual avg. temp." = mean(Value))
```

The [Battle of Berlin](https://en.wikipedia.org/wiki/Battle_of_Berlin) was in the spring of 1945, so there's really no question as to why the data is missing. I could go to the trouble of interpolating values from the flanking years, but I that wouldn't affect the overall trends at all, so I'll just drop 1945 instead and save myself some time. I'll also aggregate the station data to make later visualizations a little easier.

```{r drop 1945}
berlin_data <- berlin_data <- filter(berlin_data, Year != 1945) %>% 
    group_by(Year, Month, Week, Day, Season, Date, Variable) %>% 
    summarise(Value = mean(Value, na.rm = T)) %>% 
    ungroup()
```

The next chart compares changes in Berlin's average yearly temperature with global temperature shifts. First, I need to download some data of global historical temperatures from [NASA](https://climate.nasa.gov/vital-signs/global-temperature/).

```{r dowload nasa temp data}
nasa_data <- read_tsv("https://climate.nasa.gov/system/internal_resources/details/original/647_Global_Temperature_Data_File.txt",
                      col_names = c("Year", "World", "lowess"),
                      col_types = "idd"
                      )
```


I can then combine the NASA data with the Berlin data and visualize them together. The following graph shows the trend in mean global temperatures compared to Berlin. Note that the y-axis displays average annual temperature *change* in Â°C, and *not* average annual temperature. What I call change NASA calls **annual average anomaly**, which is the "change in global surface temperature relative to 1951-1980 average temperatures". To maintain consistency with NASA's data I'll also transform the Berlin data to represent the change from the 1951-1980 average. Also note that I plot Loess curves with `span = 0.2` to cut out some of the noise.

```{r viz global and local temp change}
berlin_data %>%
    filter(Variable == "Avg. Temp. (Â°C)",
           ) %>%
    group_by(Year) %>%
    summarise(Value = mean(Value, na.rm = T)) %>%
    
    # put Berlin data in relation to 1951-1980 average
    mutate(Berlin = Value - mean(filter(.data = .,
                                        Year %in% 1951:1980
                                        )[["Value"]]
                                 )
           ) %>%  
    right_join(nasa_data, by = "Year") %>%
    select(Year, Berlin, World) %>%
    filter(!is.na(Berlin), !is.na(World)) %>% 
    
    # loess transform
    mutate(Berlin = predict(loess(Berlin ~ Year, data = ., span = 0.2)),
           World = predict(loess(World ~ Year, data = ., span = 0.2))
           ) %>% 
    
    # tidy and plot
    gather(key = "Scope", value = "Value", 2:3) %>%
    ggplot(aes(x = Year, y= Value)) +
    geom_line(aes(color = Scope), size = 1) +
    geom_hline(yintercept = 0, linetype = 8) +
    thm + 
    scale_x_continuous(limits = c(1876, 2017),
                       breaks = seq(1877, 2017, by = 10)
                       ) +
    scale_y_continuous(limits = c(-1, 2),
                       breaks = seq(-1, 2, .5)) +
    labs(color = "",
         title = "Temp. Change in Â°C (LOESS), World vs. Berlin, 1877-2017",
         y = "Change (Â°C)"
         )
```

The upward trend in mean annual temperatures in Berlin broadly parallels that of the global aggregate. Interestingly, temperatures seem to be increasing faster in Berlin: average annual temperatures have increased by about 2Â°C since 1925, compared to a 1Â°C globally.

The final four plots 

```{r}
filter_group <- function(df){
    df %>% filter(Month %in% c("May", "June", "July", "August", "September"),
                  Year > 1956
                  ) %>%
    group_by(Year, Month)
    }
    
gg_geoms_x_scale <- function(df){
    df %>% ggplot(aes(x = Year, y = Value, color = Month)) +
        geom_line(alpha = .3) +
        geom_smooth(method = "loess", span = 0.5, se = F) +
        scale_x_continuous(limits = c(1957, 2017),
                           breaks = seq(1957, 2017, by = 10)
                           )
    }
```


```{r temperatures greater that 25C}
berlin_data %>% 
    filter(Variable == "Max Temp. (Â°C)") %>%
    filter_group() %>% 
    summarise(Value = sum(Value > 25)/n()) %>% 
    gg_geoms_x_scale() +
    scale_y_continuous(limits = c(0, 1),
                       breaks = seq(0, 1, by = .1)) +
    thm + 
    labs(title = "Proportion of Days with Temp. > 25Â°C, 1957-2017",
         y = "Proportion"
         )
```
The spike in July corresponds with the [2006 European heat wave](https://en.wikipedia.org/wiki/2006_European_heat_wave).

```{r avg. daily sunshine by month}
berlin_data %>% 
    filter(Variable == "Sun Shine Duration (h)") %>%
    filter_group() %>% 
    summarize(Value = mean(Value)) %>% 
    gg_geoms_x_scale() +
    scale_y_continuous(limits = c(2.4, 12.5),
                       breaks = seq(2.5, 12.5, by = 1)
                       ) +
    thm +
    labs(title = "Avg. Daily Sunshine (Hours), 1957-2017",
         y= "Duration (h)"
         )
```

```{r avg. daily precipitation by month}
berlin_data %>% 
    filter(Variable == "Precipitation (mm)") %>%
    filter_group() %>% 
    summarize(Value = mean(Value)) %>% 
    gg_geoms_x_scale() + 
    scale_y_continuous(limits = c(-0.5, 7.5),
                       breaks = seq(0, 7.5, by = 1)) +
    thm + 
    labs(title = "Avg. Daily Precipitation (mm), 1957-2017",
         y= "Amount (mm)"
         )
```

```{r proportion days without precipitation by month}
berlin_data %>% 
    filter(Variable == "Precipitation (mm)") %>%
    filter_group() %>% 
    summarize(Value = sum(Value == 0)/n()) %>% 
    gg_geoms_x_scale() + 
    scale_y_continuous(limits = c(0.1, .95),
                       breaks = seq(0.1, .9, by = .1)) +
    thm + 
    labs(title = "Days Without Precipitation (Proportion), 1957-2017",
         y= "Proportion"
         )
```


---
title: How About That Weather!? - An EDA of Climate Change in Berlin.
author: Gershom Tripp
date: '2018-09-17'
slug: how-about-that-weather-an-eda-of-climate-change-in-berlin
categories:
  - R
tags:
  - Berlin
  - climate
  - EDA
output:
  blogdown::html_page:
    toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```










## Introduction

The following exploratory data analysis of climate change in Berlin represents a first step in bringing this often abstract issue closer to home. Why is this necessary? Per Espen Stoknes writes the following about the role of psychological "distance" when it comes to inaction in the face of climate change in his book [*What We Think About When We Try Not to Think About Global Warming*](https://www.amazon.de/What-Think-About-Global-Warming/dp/1603585834):

> The climate issue remains remote for the majority of us, in a number of ways. We can’t see climate change. Melting glaciers are usually far away, as are the spots on earth now experiencing sea level rise, more severe floods, droughts, fires, and other climate disruptions. It may hit foreign others, not me or my kin. And the heaviest impacts are far off in time—in the coming century or farther. Despite some people stating that global warming is here now, it still feels distant from everyday concerns.

There is little doubt that climate change is happening [now](https://climate.nasa.gov/effects/), and there's some evidence that it's accelerating even [faster than predicted](https://www.theguardian.com/world/2018/aug/21/arctics-strongest-sea-ice-breaks-up-for-first-time-on-record). I am fairly certain that I have witnessed climate change in action in the 11+ years that I have lived in Berlin

For the record, I'm neither a climatoligist nor a meteorologist, and nothing about my analysis is going to be cutting edge or terribly original. I'll avoid making too many inferences based on the data, focussing instead on *exploration* using fairly basic statistical methods. That being said, I should still be able draw some conclusions, however tentative. Also, since this is really just a *first step*, there's a good chance that I'll revisit this topic at a later date and build on what I've done here.

The next couple of sections cover how I wrangle the data into a more manageable form. There's little to see other than code. If you're not interested in that kind of thing just [skip to the analysis](#visualizing-climate-change-in-berlin).




***




## Data Import and Preparation

First off, the libraries:

```{r load-libs, warning=FALSE, error=FALSE, message=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(stats)
library(psych)
library(lubridate)
library(ggplot2)
library(corrplot)
```



### Importing

The data is a subset of the [Deutscher Wetterdienst](https://www.dwd.de/DE/leistungen/klimadatendeutschland/klarchivtagmonat.html)'s
extensive repository of weather data. It's possible to get hourly measurements, going back decades, but I decided daily averages were enough for this analysis. Downloading the data programmatically from the DWD archive interface would have been horribly complex, so I did it the old-fashioned way: I downloaded the data as four ZIPs (from four Berlin weather stations), extracted the TXT/CSV files, renamed them to something more descriptive, and wrote some code to remove some excess white space that was interfering with the data import.

> **Edit**: While putting the final touches on this post I discovered that the DWD offers FTP access to their databases via the [Climate Data Center](ftp://ftp-cdc.dwd.de/pub/CDC/). I would definitely take this route in future analyses.

The data files are semicolon-delimited. To keep things simple I'll import everything as character strings. Numeric and date objects are converted later, as needed. The value `-999` is imported as `NA`.

```{r remove-whitespace, message=FALSE}
# get the names of  the data files, import them as text, remove white space and
# overwrite originals with de-whitespaced files
data_path <- file.path("datasets", "berlin_climate_data")
file_names <- dir(data_path)
file_names <- file_names[grep(pattern = "data", file_names)]

for (file_name in file_names){
    file_path <- file.path(data_path, file_name)
    file <- read_file(file_path)
    file <- gsub(" ", "", file)
    write_file(file, file_path)
}

# wrapper function to load data with defaults
read_data <- function(path){
    read_delim(file = path,
               delim = ";",
               col_types = paste0(rep("c", 19), collapse = ""),
               na = "-999"
               )
}

# read in the data
dahlem_fu_data <- read_data(file.path(data_path, "Dahlem-(FU)-data.txt"))
dahlem_lfag_data <- read_data(file.path(data_path, "Dahlem-(LFAG)-data.txt"))
tegel_data <- read_data(file.path(data_path, "Tegel-data.txt"))
tempelhof_data <- read_data(file.path(data_path, "Tempelhof-data.txt"))
```


It makes sense to take a look at the included parameter files to get an idea what kind of data I'm working with. These are in German (umlauts!), so I'll use `guess_encoding()` from the `readr` library to figure out which ISO I need.

```{r guess-encoding}
guess_encoding("datasets/berlin_climate_data/Dahlem-(FU)-params.txt") %>% 
    knitr::kable()
```

ISO-8859-1 it is. I don't care too much about the details of each station, so I'll just import one file, `Tempelhof-params.txt`, to gain some insight into the parameters, i.e. abbreviations, units used, etc. 

```{r load-show-params, warning=FALSE}
# load parameter file
data_path <- file.path("datasets", "berlin_climate_data", "Tempelhof-params.txt")
tempelhof_params <- read_delim(file = data_path,
                               delim = ";",
                               col_types = paste0(rep("c", 13), collapse = ""),
                               locale = locale(encoding = "ISO-8859-1")
                               )

# filter and show parameters
tempelhof_params %>%
    select(2:7) %>% 
    filter(!duplicated(Parameter) & !is.na(Parameter)) %>% 
    knitr::kable()
```





### Tidying

There are a couple more changes to be made before I can start visualizing. First, [tidyverse](https://www.tidyverse.org/) magic works most potently on *long* data, but the DW uses *wide* format ([see the difference here](https://en.wikipedia.org/wiki/Wide_and_narrow_data)). Second, the variable abbreviations are hard to understand (FM, RSK, etc.), so I should replace them with something else. Third and last, I now have four datasets, one for each Berlin weather station, but one would be preferable.

I'll change the variable names using the following map.

```{r names-map}
var_names <- c("Date" = "MESS_DATUM",
               "Avg. Wind Speed (m/sec)" = "FM",
               "Max Wind Speed (m/sec)" = "FX",
               "Avg. Cloud Cover (8th)" = "NM",
               "Avg. Air Pressure (hPa)" = "PM",
               "Precipitation (mm)" = "RSK",
               "Sun Shine Duration (h)" = "SDK",
               "Snow (cm)" = "SHK_TAG",
               "Avg. Temp. (°C)" = "TMK",
               "Min. Temp. (°C)" = "TNK",
               "Max Temp. (°C)" = "TXK",
               "Avg. Rel. Humidity (%)" = "UPM"
               )
```

The following `tidy_data` function reduces each data frame to the set of variables I'm interested in, converts the character strings to numbers and dates, and `gathers` the variables into long format.

```{r function-tidy-data}
tidy_data <- function(df, vars, station_name){
    df <- select(df, vars)
    df <- as_tibble(sapply(df, as.double))
    df$Date <- ymd(df$Date)
    df <- gather(df, key = "Variable", value = "Value", 2:12)
    df$Station <- station_name
    df
}
```

I can now call `tidy_data` for each station, and then combine the station data into one data frame `berlin_data`.

```{r tidy-stations}
# call tidy_data for each station dataset
dahlem_lfag_data <- tidy_data(dahlem_lfag_data, var_names, "Dahlem (LFAG)")
dahlem_fu_data <- tidy_data(dahlem_fu_data, var_names, "Dahlem (FU)")
tegel_data <- tidy_data(tegel_data, var_names, "Tegel")
tempelhof_data <- tidy_data(tempelhof_data, var_names, "Tempelhof")

# combine station data into one data frame
berlin_data <- rbind(dahlem_lfag_data, dahlem_fu_data, tegel_data, tempelhof_data)
```

There is just one final change I would like to make: The values reflect daily averages, and measurement began in the 19th century for some stations, so I'm faced with hundreds of thousands of observations! To make things more manageable I'll include different time scales, e.g. years, months, weeks, days, and [meteorological seasons](https://en.wikipedia.org/wiki/Season#Meteorological). While I'm at it I'll also convert months and seasons to factors.

```{r add-time-units}
# add years, months, weeks, days, and seasons
berlin_data <- berlin_data %>% 
    mutate(Year = year(Date),
           Month = month(Date),
           Week = week(Date),
           Day = day(Date),
           Season = case_when(Month %in% c(12, 1, 2) ~ "Winter",
                              Month %in% 3:5 ~ "Spring",
                              Month %in% 6:8 ~ "Summer",
                              Month %in% 9:11 ~ "Fall",
                              )
           )

# turn seasons into factors
berlin_data$Season <- factor(berlin_data$Season,
                             ordered = T,
                             levels = c("Spring", "Summer", "Fall", "Winter")
                             )
berlin_data$Station <- factor(berlin_data$Station,
                             ordered = T,
                             levels = c("Dahlem (LFAG)", "Dahlem (FU)", "Tempelhof", "Tegel")
                             )

# turn months into factors
berlin_data$Month <- factor(berlin_data$Month,
                            ordered = T,
                            levels = 1:12,
                            labels = c("January", "February", "March", "April",
                                       "May", "June", "July", "August", 
                                       "September", "October", "November",
                                       "December"
                                       )
                            )
```











***










## Visualizing Climate Change in Berlin

I now have a few hundred thousand observations for each of four Berlin weather stations, which I can freely group based on my analytical needs. The following table shows the total observations (daily averages) for each of Berlin's four weather stations.

```{r show-observations}
berlin_data %>% 
    group_by(Station) %>% 
    summarise("number of observations" = n()) %>% 
    knitr::kable()
```











### Average Annual Temperature Change: Berlin

My first question is: How have average annual temperatures changed since 1876? The following time series shows that temperatures in Berlin have increased by about 2°C, on average, with the warming rate apparently accelerating from about the middle of the last century. There is quite a bit of variation, but the general trend is unmistakable. 
 
```{r viz-yearly-temps, message=FALSE}
# create variable to hold theme (reusable)
thm <- theme_minimal() + 
    theme(panel.grid.minor = element_blank(),
          panel.grid.major.x = element_blank()
          )

# generate combined timeseries of temperatur data
berlin_data %>% 
    filter(Variable == "Avg. Temp. (°C)") %>%
    group_by(Station, Year) %>% 
    summarise(Value = mean(Value, na.rm = T)) %>%
    ggplot(aes(x = Year, y = Value)) +  
    geom_line(aes(color = Station)) +
    scale_color_manual(values = c("#000000", "#000090", "#4848ED", "#8888FF")) +
    geom_smooth(se = F, method = "loess", span = .8, color = "#FF0000") +
    labs(title = "Avg. Yearly Temperature (°C), 1877-2017",
         y = "Temp."
         ) +
    thm +
    scale_x_continuous(limits = c(1876, 2017),
                       breaks = seq(1877, 2017, by = 10)
                       ) +
    scale_y_continuous(limits = c(3, 12),
                       breaks = seq(3, 12, by = 1)
                       )
```

Also conspicuous is the precipitous drop in 1945. Histograms of each year from 1941-1949 reveal bimodal distributions of daily temperatures for most years. One mode is missing from 1945, that is to say, some data is missing, and the data that is available clusters around the low end.

```{r viz-temps-distribution}
berlin_data %>% 
    filter(Station == "Dahlem (LFAG)",
           Variable == "Avg. Temp. (°C)",
           Year %in% 1941:1949,
           !is.na(Value)
           ) %>% 
    ggplot(aes(x = Value)) + 
    geom_histogram(bins = 30) +
    facet_wrap(~Year) + 
    theme_minimal() + 
    theme(panel.grid.minor = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.spacing = unit(2, "lines")
          ) +
    labs(title = "Distribution of Annual Temperatures (°C), 1941-1949",
         x = "Temperature"
         )
```

A table of observation counts (i.e. number of daily averages) confirms that 1945 is missing data.

```{r table-temps-distribution}
berlin_data %>% 
    filter(Station == "Dahlem (LFAG)",
           Variable == "Avg. Temp. (°C)",
           Year %in% 1941:1949,
           !is.na(Value)
           ) %>% 
    group_by(Year) %>% 
    summarize("number of obs." = n(),
              "annual avg. temp." = mean(Value)) %>% 
    knitr::kable()
```

The [Battle of Berlin](https://en.wikipedia.org/wiki/Battle_of_Berlin) was in the spring of 1945, so there's really no question as to why the data is missing. I could go to the trouble of interpolating values from the flanking years, but I'll save myself some time and just drop 1945 instead, which shouldn't have any effect on the overall trend. I'll also aggregate the station data to make later visualizations a little easier.

```{r drop-1945}
berlin_data <- filter(berlin_data, Year != 1945) %>% 
    group_by(Year, Month, Week, Day, Season, Date, Variable) %>% 
    summarise(Value = mean(Value, na.rm = T)) %>% 
    ungroup()
```




### Average Annual Temperature: Global vs. Local


The next chart compares changes in Berlin's average yearly temperature with global temperature shifts. First, I need to download some data of global historical temperatures from [NASA](https://climate.nasa.gov/vital-signs/global-temperature/).

```{r get-nasa-data}
nasa_data <- read_tsv("https://climate.nasa.gov/system/internal_resources/details/original/647_Global_Temperature_Data_File.txt",
                      col_names = c("Year", "World", "lowess"),
                      col_types = "idd"
                      )
```


I can then combine the NASA data with the Berlin data and visualize them together. The following graph shows the trend in global annual means compared to Berlin's. Note that the y-axis displays average annual temperature *change* in °C, and *not* average annual temperature. What I call change NASA calls *annual average anomaly*, which is the "change in global surface temperature relative to 1951-1980 average temperatures". To maintain consistency with NASA's data I'll also transform the Berlin data to also represent the change from the 1951-1980 average. Also note that I plot Loess curves (`span = 0.2`) to cut out the noise.

```{r viz-global-vs-berlin}
berlin_data %>%
    filter(Variable == "Avg. Temp. (°C)",
           ) %>%
    group_by(Year) %>%
    summarise(Value = mean(Value, na.rm = T)) %>%
    
    # put Berlin data in relation to 1951-1980 average
    mutate(Berlin = Value - mean(filter(.data = .,
                                        Year %in% 1951:1980
                                        )[["Value"]]
                                 )
           ) %>%  
    right_join(nasa_data, by = "Year") %>%
    select(Year, Berlin, World) %>%
    filter(!is.na(Berlin), !is.na(World)) %>% 
    
    # loess transform
    mutate(Berlin = predict(loess(Berlin ~ Year, data = ., span = 0.3)),
           World = predict(loess(World ~ Year, data = ., span = 0.3))
           ) %>% 
    
    # tidy and plot
    gather(key = "Scope", value = "Value", 2:3) %>%
    ggplot(aes(x = Year, y= Value)) +
    geom_line(aes(color = Scope), size = 1) +
    geom_hline(yintercept = 0, linetype = 8) +
    thm + 
    scale_x_continuous(limits = c(1876, 2017),
                       breaks = seq(1877, 2017, by = 10)
                       ) +
    scale_y_continuous(limits = c(-1, 2),
                       breaks = seq(-1, 2, .5)) +
    scale_color_manual(values = c("red", "blue")) +
    labs(color = "",
         title = "Annual Avg. Anomaly (LOESS), World vs. Berlin, 1877-2017",
         y = "Change (°C)"
         )
```

The upward trend in mean annual temperature in Berlin broadly parallels that of the global aggregate. Interestingly, the annual mean seems to be increasing faster in Berlin than it is globally - approximately 2°C vs. 1°C.  This is likely due to the [urban heat island effect](https://en.wikipedia.org/wiki/Urban_heat_island), and should be considered in future analyses.






### 

The rest of the this analysis focusses on the last six decades, for two reasons: First, most of the increase in annual average temperature occured within that time period, so I think any consequent shifts in other variables will probably occur then, too. Second, data from the prior period come from only one station, are partially incomplete, and some variables simply weren't measured before about 1957, e.g. sunshine duration and wind speed.

I'll also stick to Loess curves 
```{r subset-viz-all-lm, fig.width=10, fig.height=5}
berlin_subset <- berlin_data %>% 
    filter(Year > 1956, complete.cases(.)) %>% 
    filter(Variable %in% c("Avg. Air Pressure (hPa)",
                           "Avg. Cloud Cover (8th)",
                           "Avg. Rel. Humidity (%)",
                           "Avg. Temp. (°C)",
                           "Avg. Wind Speed (m/sec)",
                           "Precipitation (mm)",
                           "Snow (cm)",
                           "Sun Shine Duration (h)"
                           )
           )

berlin_subset %>% 
    group_by(Year, Variable) %>%  
    summarize(Value = mean(Value, na.rm = T)) %>% 
    ggplot(aes(x = Year, y = Value)) +
    geom_line(alpha = 0.5) +
    geom_smooth(method = "lm", se = F) +
    facet_wrap(~ Variable, scales = "free", ncol = 4) +
    thm +
    scale_x_continuous(breaks = seq(1957, 2017, by = 10))
```


```{r subset-viz-all-loess, fig.width=10, fig.height=5}
berlin_subset %>% 
    group_by(Year, Variable) %>%  
    summarize(Value = mean(Value, na.rm = T)) %>% 
    ggplot(aes(x = Year, y = Value)) +
    geom_line(alpha = 0.5) +
    geom_smooth(method = "loess", se = F) +
    facet_wrap(~ Variable, scales = "free", ncol = 4) +
    thm +
    scale_x_continuous(breaks = seq(1957, 2017, by = 10))
```



```{r subset-viz-summer-loess, fig.width=10, fig.height=5}
berlin_subset %>% 
    filter(Month %in% c("May", "June", "July", "August")) %>% 
    group_by(Year, Variable) %>%  
    summarize(Value = mean(Value, na.rm = T)) %>% 
    ggplot(aes(x = Year, y = Value)) +
    geom_line(alpha = 0.5) +
    geom_smooth(method = "loess", se = F) +
    facet_wrap(~ Variable, scales = "free", ncol = 4) +
    thm +
    scale_x_continuous(breaks = seq(1957, 2017, by = 10))
```



### Proportion of +25°C Days

Next, I would like to see how the increase in average annual temperatures has affected the high end of the overall distribution of annual temperatures. The following bit of code just stores some useful, reusable settings.

```{r save-settings}
filter_group <- function(df){
    df %>% filter(Year %in% (1957:2017)) %>%
    group_by(Year, Season, Month)
    }
    
gg_geoms_x_scale <- function(df){
    df %>% ggplot(aes(x = Year, y = Value, color = Season)) +
        geom_line(alpha = .5) +
        geom_smooth(method = "loess", se = F) +
        scale_color_manual(values = c("#FF9F9F", "#E50000", "#7777FF", "#0000C4")) +
        scale_x_continuous(limits = c(1957, 2017),
                           breaks = seq(1957, 2017, by = 10)
                           ) +
        facet_wrap(~ Month)
}

thm <- thm + 
    theme(panel.spacing = unit(1, "lines"))
```

Now to plot the *proportion* of days per month, for the months May-September, where maximum temperatures exceed 25°C. The colors correspond to [meteorological season](https://en.wikipedia.org/wiki/Season#Meteorological), with spring and summer being shades of red, and fall and winter shades of blue.

```{r viz-temps-25, fig.height=10, fig.width=10}
berlin_data %>% 
    filter(Variable == "Max Temp. (°C)"
           ) %>%
    filter_group() %>% 
    summarise(Value = sum(Value > 25)/n()) %>% 
    gg_geoms_x_scale() +
    scale_y_continuous(limits = c(-0.5, 1),
                       breaks = seq(0, 1, by = .1)
                       ) +
    thm +
    labs(title = "Proportion of Days with Temp. > 25°C, 1957-2017",
         y = "Proportion"
         )
```

Predictably, the proportion of days with maximum temperatures above 25°C has gone up since 1957, particularly in July and August, but also in May and September. The summer months show a great deal of variation. Also interesting is the fact that June has remained fairly stable even as other months show dramatic increases. This could provide one explanation, via a sort of [contrast effect](https://en.wikipedia.org/wiki/Contrast_effect), for the perception that June's weather is getting worse, although I'm not so sure this 





### Sunshine

Looking at the next plot, the average number of hours of daily sunshine appears to have remained fairly stable at least since 1977. There is some fluctuation, but there is no clear trend in any direction. I take this to mean that these months have not become significantly more overcast (or, conversely, sunnier) - at least, not over the long term.

```{r viz-sunshine, fig.height=10, fig.width=10}
berlin_data %>% 
    filter(Variable == "Sun Shine Duration (h)") %>%
    filter_group() %>% 
    summarize(Value = mean(Value)) %>% 
    gg_geoms_x_scale() +
    scale_y_continuous(limits = c(0, 12.5),
                       breaks = seq(0, 12, by = 1)
                       ) +
    facet_wrap(~ Month) +
    thm +
    labs(title = "Avg. Daily Sunshine (Hours), 1957-2017",
         y= "Duration (h)"
         )
```






### Precipitation

For the most part the average amount of precipitation also doesn't show any general upward trend since 1957. At shorter timescales, however, and for individual months, there are discernible trends. June and July - and to a lesser degree, October - show a clear increase in the amound of precipitation. 

```{r viz-precipitation, fig.height=10, fig.width=10}
berlin_data %>% 
    filter(Variable == "Precipitation (mm)") %>%
    filter_group() %>% 
    summarize(Value = mean(Value)) %>% 
    gg_geoms_x_scale() + 
    scale_y_continuous(limits = c(0, 8),
                       breaks = seq(0, 8, by = 1)) +
    facet_wrap(~ Month) +
    thm + 
    labs(title = "Avg. Daily Precipitation (mm), 1957-2017",
         y= "Amount (mm)"
         )
```







### Days Without Precipitation

Changes in the proportion of "wet" to "dry" days since 1957 depend on the timescale used, as well as the month being analyzed. 

```{r viz-no-precipitation, fig.height=10, fig.width=10}
berlin_data %>% 
    filter(Variable == "Precipitation (mm)") %>%
    filter_group() %>% 
    summarize(Value = sum(Value == 0)/n()) %>% 
    gg_geoms_x_scale() + 
    scale_y_continuous(limits = c(0, 1),
                       breaks = seq(0, 1, by = .1)) +
    thm +
    labs(title = "Days Without Precipitation as Proportion of all Days, 1957-2017",
         y= "Proportion"
         )
```







### Air Pressure

Given what I know about the dynamic between air pressure and weather - which is admittedly not that much - I think air pressure could be a further indicator of storm intensity. As I understand it, stable high pressure zones usually indicate good weather while rapidly decreasing air pressure indicates an impending storm. Hourly data are necessary to properly analyze the association between air pressure and bad weather, something I decided against for the this analysis for time reasons. Still, I assume strong storms would leave some trace by driving down daily averages. 

To see if air pressure, measured in hectopascals (hPa), is a good indicator of weather conditions, I'll plot the correlations of some relevant variables in a correlogram. I should note that this probably isn't the best method to determine how air pressure affects weather. It's more of a heuristic to help me determine if there is a question here worth answering. Finer-grained air pressure data and variables more closely associated with bad weather, cloud type for instance, in addition to more rigorous statistical methods would allow me to more confidently draw conclusions. But that's a project for another day.

First I need to compute a correlation matrix, which requires some transformation. This code *widens* the data by giving each variable it's own column (basically the reverse of the above *tidying* process), and then computes the correlations (Spearman's $\rho$ in this case).

```{r transform-subset-correlation}
berdat_sub <- berlin_data %>%
    filter(Month %in% c("May", "June", "July", "August"),
           Year > 1956) %>%
    spread(Variable, Value) %>% 
    select(7:17, -`Snow (cm)`, -`Avg. Cloud Cover (8th)`)

names(berdat_sub) <- c("PRESS", "HUMID", "TEMP_AVG", "WIND_AVG", "TEMP_MAX",
                       "WIND_MAX", "TEMP_MIN", "PRECIP", "SUN")

berdat_cortest <- corr.test(x = berdat_sub, method = "spearman")
```

I'll use a matrix of p-values, conveniently calculated by the `psych` library's `corr.test` function, to test for the statistical significance of the individual correlations, as even low correlation can hint at so sort of association. The significance level is set to $\alpha$ = 0.001. (**Note**: An outcome is "statistically significant" when the probability of a chance occurence, given by a p-value, is less than $\alpha$, i.e. *very* small.)

```{r viz-correlogram, fig.width=7, fig.height=7}
corrplot(berdat_cortest$r,
         p.mat = berdat_cortest$p,
         sig.level = .001,
         type = "upper",
         method = "number",
         order = "AOE",
         tl.cex = .8,
         title = "Correlogram (rho) of Selected Variables",
         mar = c(0, 0, 2, 0),
         col = colorRampPalette(c("red", "white", "blue"))(100)
         )
```


Of particular interest are correlations between air pressure (`PRESS`) and the other variables that also make intuitive sense, as that would imply that air pressure is a good predictor of weather conditions. We should expect, for example, that high pressure is positively correlated with fair weather, given by sunshine duration (`SUN`), and negatively correlated with wind and bad weather, as given by maximum and average windspeed (`WIND_MAX` and `WIND_AVG`) and the amount of precipitation (`PRECIP`). Looking at the plot, this seems to be the case, so I'll accept - for the time being, and with the above proviso - that a day's average air pressure can tell me something about the weather conditions for that day.

The final plot displays the number of days per month where there is at least a -3 hPa *drop* in air pressure compared to the previous day. Although this measure is obviously not perfect, my layperson intuition tells me that an increase in the number of such drops might hint at an increase in storm fronts of a certain intensity.

The choice of a -3 hPa drop is not completely arbitrary. A convenient table on [bohlken.net](http://www.bohlken.net/airpressure2.htm) (a barograph maker) states that a drop of -3 hPa can lead to strong winds of 6-7 Bfts (11-17 m/s), so it seems like a decent place to start.

Nevertheless, there are at least two major problems here to keep in mind: First, the table on bohlken.net relates wind speed increases to a drop in air pressure over a 1-3 hour timeframe, while the data I'm using contains 24-hour means. Second, there is an implicit assumption that "bad weather" days and "good weather" days are more or less evenly dispersed, as there wouldn't otherwise be any difference between consecutive days of the same type. This is very likely not the case.

Still, I'm fairly confident that larger, more extreme storm fronts would leave a trace on daily hPa averages, and that an increase in the amount or intensity of these fluctuations is measurable. I also think that severe weather is still infrequent enough in Berlin that an increasing number of large drops should show up in monthly aggregates.

The below plot seems to confirm my intuitions. Especially the months May-August show strong upward trends over the last 10-15 years, which one might interpret as an increase in the number of storms of a certain magnitude. This is also consistent with my understanding of how temperature, air pressure, and storm systems relate: The warmer the air, the more water vapor it contains, and the more intense the resulting storms.


```{r viz-pressure-drop, fig.height=10, fig.width=10}
berlin_data %>% 
    filter(Variable == "Avg. Air Pressure (hPa)") %>% 
    filter_group() %>% 
    mutate(Value = (Value - lag(Value))) %>%
    filter(!is.na(Value), Value < -3) %>%
    group_by(Year, Season, Month) %>%
    summarize(Value = n()) %>%
    gg_geoms_x_scale() + 
    facet_wrap(~ Month) +
    scale_y_continuous(limits = c(2, 15),
                       breaks = seq(2, 15, by = 1)
                       ) +
    thm + 
    labs(title = "Drop in Air Pressure (hPa) from Previous Day > 3",
         y= "Number of Days"
         )
```

It should be noted that this trend is confined to the last 10 to 15 years. 






## Conclusion

If there is one conclusion that I can confidently draw from the above analysis, it is that Berlin is getting warmer. It is also [well known](https://earthobservatory.nasa.gov/Features/ClimateStorms/page2.php) that temperature has an effect on weather systems generally, and storm intensity specifically. Although I can't say conclusively, based on the data, that increasing temperatures are causing changes of this type in Berlin, there are are some interesting patterns that seem to hint at just this.

Taken together, shifts in the amount of summer precipitation,  *suggest* that Berlin is experiencing a 10 to 20 year trend of more extreme weather: Hotter summers punctuated by more intense summer storms. I've deduced this from the following:

    * 





